
import logging
import os
import shutil
import tempfile
import pickle
from collections import namedtuple
from pathlib import Path
import numpy as np
import subprocess
import isodesign

import pandas as pd
from influx_si import C13_ftbl, txt2ftbl

from isodesign.base.isotopomer import Isotopomer
from isodesign.base.label_input import LabelInput
from isodesign.base.score import ScoreHandler

logger = logging.getLogger(f"IsoDesign.{__name__}")  

# Namedtuples are placed outside the class to avoid pickling issues when saving the Process object
# namedtuple containing file path and data 
file_info = namedtuple("file_info", ['path', 'data']) 
# namedtuple containing the number of labeled inputs and the total price for each linp file
linp_info = namedtuple("linp_info", ["nb_labeled_inputs", "total_price"])

class Process:
    """
    The Process class is the main class to organise IsoDesign functionalities.
    Key features:
    - importing and reading data files
    - generating combinations of isotopomers that are stored in influx_si .linp files
    - runs simulations with influx_s to get predicted fluxes and associated SDs
    - generates a summary.

    """
    FILES_EXTENSION = [".netw", ".tvar", ".mflux", ".miso", ".cnstr", ".mmet", ".opt"]

    def __init__(self):
        # Version of the isodesign package
        self.isodesign_version = isodesign.__version__
        # Dictionary to store imported file contents. Key : files extension,
        # value : namedtuple with file path and contents
        self.mtf_files = {}
        # LabelInput object
        # To use the generate_labelling_combinations method
        self.label_input = None

        # Dictionary containing element to build the vmtf file
        self.vmtf_element_dict = {"Id": None, "Comment": None}
        self.netw_directory_path = None
        # Path to the folder containing all the model to analyze
        self.model_directory_path = None
        # Path to the folder containing the results of the analysis
        self.output_folder_path = None
        # Path to the folder containing the files created by Isodesign
        self.tmp_folder_path = None
        # Name of the model to analyze
        self.model_name = None
        # List of paths to tvar.sim files (after simulation with influx_si)
        self.tvar_sim_paths = []
        # Elements analyzed (substrates, metabolites, etc.) in the network using the analyse_model method
        self.netan = {}
        # summary dataframe generated after simulation with influx_si
        self.summary_dataframe = None
        # filtered dataframe after filter use
        self.filtered_dataframe = None
        # Dictionary containing isotopomers
        # key : substrate name, value : list of isotopomers
        self.isotopomers = {}
        # # Get the tvar.def file generated during the model analysis
        # self.tvar_def_file = None
        # Dictionary to store the number of labeled inputs and the total isotopomer prices of each linp file 
        # Key : linp file name, value : namedtuple containing the number of labeled inputs and the total price
        self.linp_infos = {}
        # Dictionary containing the number of structurally identified fluxes (obtained from the ‘tvar.sim’ files) 
        # Key : file name, value : number of structurally identified fluxes
        self.structures_identified = {}
        # List storing linp file data in the form of dictionaries
        # each dictionary representing a combination of labelled substrates.
        self.linp_dataframes = {}
        # List of command line arguments to pass to influx_si
        self.command_list = None
        # List of linp dataframes to remove
        self.linp_to_remove = {}

        # Stores the scores generated after application of the scoring criteria 
        self.scores : pd.DataFrame = None
        # Stores all the scores generated by the user
        self.all_scores = {}   
        # All this attributes are used to store the results of the analysis 
        self.filters : dict = None
        self.selected_criteria : list = []
        self.criteria_parameters : dict = None
        self.applied_operations = None

    def get_path_input_netw(self, netw_directory_path):
        """
        Get the directory path of the netw file (essential file containing 
        all reactions and transition labels). From this path, we also store 
        the directory path and the name of the model to be analyzed. 

        :param netw_directory_path: str containing the path to the netw file 
        """
        
        if not isinstance(netw_directory_path, str):
            msg = (f'"{netw_directory_path}" should be of type string and not {type(netw_directory_path)}.')
            logger.error(msg)
            raise TypeError(msg)

        self.netw_directory_path = Path(netw_directory_path)

        if not self.netw_directory_path.exists():
            msg = f"{self.netw_directory_path} doesn't exist."
            logger.error(msg)
            raise ValueError(msg)
        
        if self.netw_directory_path.suffix != ".netw":
            msg = f'{self.netw_directory_path} is invalid. Please provide a file with ".netw" extension.'
            logger.error(msg)
            raise ValueError(msg)
        
        # Store the model name 
        self.model_name = Path(netw_directory_path).stem
        
        # Store the model directory path 
        self.model_directory_path = Path(netw_directory_path).parent
        # Output folder path is the same as the model directory path
        # It can be changed by the user 
        self.output_folder_path=self.model_directory_path
    
    def load_model(self):
        """ 
        Load MTF files depending on the model name.

        """
        # Reset the dictionary to store imported files
        self.mtf_files = {}
    
        for file in self.model_directory_path.iterdir():
            if file.stem == self.model_name and file.suffix in self.FILES_EXTENSION:
                # Read the file and store its content in a namedtuple
                data = pd.read_csv(str(file),
                                    sep="\t",
                                    comment="#",
                                    header=None if file.suffix == ".netw" else 'infer',
                                    encoding="utf-8",
                                    on_bad_lines='skip')
                
                self.mtf_files.update({file.suffix[1:]: file_info(file, data)})

        logger.debug(f"Imported files = {self.mtf_files}\n")

   

    def analyse_model(self):
        """
        Analyze model network to identify substrates, metabolites, etc by using 
        modules from influx_si.
        """

        # Reset self.netan to a new empty dictionary
        # Useful if you want to reuse the function for another prefix
        self.netan = {}
        # Use a temporary directory to store files generated by the modules
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)

            # Copy all files from model_directory_path to the temporary directory
            files_to_copy = [f for f in self.model_directory_path.iterdir() if f.is_file()]
            for file in files_to_copy:
                shutil.copy(file, tmpdir_path)

            # will contain the paths to the ftbl files generated by the txt2ftbl module
            li_ftbl = []  
            # convert mtfs to ftbl
            txt2ftbl.main(["--prefix", os.path.join(str(tmpdir), self.model_name)], li_ftbl)

            # # get the tvar.def file
            # for file in os.listdir(tmpdir):
            #     if file.endswith(".tvar.def"):
            #         self.tvar_def_file = pd.read_csv(os.path.join(tmpdir, file), sep="\t", comment="/")

            # parse and analyze ftbl stored in li_ftbl
            model: dict = C13_ftbl.ftbl_parse(li_ftbl[0])

            emu = False # can be advantageous to set to "True" when there are
            # only mass measurements
            fullsys = False
            case_i = False # for influx_i, must be "True"
            # analyze the model dictionary to find different network elements
            # such as substrates, metabolites...
            C13_ftbl.ftbl_netan(model, self.netan, emu, fullsys, case_i)
        logger.debug(f"self.netan dictionary keys : {self.netan.keys()}\n")
        logger.info("Network analysis finished successfully.\n")

    
    def copy_files(self):
        """
        Copy the imported files in the linp folder. 
        All the files that will be passed to influx_si have to be in the
        same folder.
        """

        logger.debug(f"Copy of the imported files to '{self.tmp_folder_path}'.\n")

        for file in self.mtf_files.values():
            # File paths are contained as first elements in the namedtuple
            # logger.debug(f"File path: {file.path}")
            shutil.copy(file.path, self.tmp_folder_path)

    def save_process_to_file(self):
        """
        Save the Process object to a pickle file in the model directory.
        """
         
        output_file_tmp = Path(self.output_folder_path, self.model_name + "_tmp.pkl")
        output_file = Path(self.output_folder_path, self.model_name + ".pkl")

        try:
            with open(output_file_tmp, 'wb') as file:
                pickle.dump(self, file)
            output_file.unlink(missing_ok=True)
            output_file_tmp.rename(output_file)
        except Exception as e:
            raise ValueError(f"An unknown error has occured when saving the process file: {e}")
    
    def configure_unlabelled_form(self):
        """
        Add the unlabelled form of the inputs to the isotopomers dictionary 
        (key: substrate name, value: list of isotopomers) with default values.

        """
        for substrates_name in self.netan["input"]:
            self.isotopomers[substrates_name] = [Isotopomer(substrates_name, 
                                                       self.netan["Clen"][substrates_name] * "0", 
                                                       intervals_nb=10, 
                                                       lower_bound=1, 
                                                       upper_bound=1, 
                                                       price=None)]

    
    def add_isotopomer(self, substrate_name, labelling, intervals_nb, lower_b, upper_b, price=None):	
        """
        Add isotopomer to the isotopomers dictionary (self.isotopomers). 

        :param substrate_name: name of the substrate
        :param labelling: labelling of the isotopomer
        :param intervals_nb: number of intervals to test
        :param lower_b: lower bound
        :param upper_b: upper bound
        :param price: price of the isotopomer. 
        """
        
        isotopomer = Isotopomer(substrate_name, labelling, intervals_nb, lower_b, upper_b, price)
        
        # Check if the labelling length is equal to the number of carbons in the substrate
        # by using clen (carbon length) from the netan (network analysis) dictionary
        if len(labelling) != self.netan["Clen"][substrate_name]:
            raise ValueError(f"Number of atoms for {substrate_name} should be equal to {self.netan['Clen'][substrate_name]}")
        # Check if the labelling already exists for the substrate 
        # each substrate must have unique isotopomers 
        if labelling in [isotopomer.labelling for isotopomer in self.isotopomers[substrate_name]]:
            # logger.error(f"Isotopomer {labelling} already exists for {substrate_name}")
            raise ValueError(f"Isotopomer {labelling} already exists for {substrate_name}")
            
        self.isotopomers[substrate_name].append(isotopomer)
            
    

    def remove_isotopomer(self, substrate, labelling):
        """
        Remove isotopomer from the isotopomer dictionary (self.isotopomers) 
        according to the substrate name and labelling.

        :param substrate: substrate name
        :param labelling: labelling for isotopomer to remove

        """
    
        for isotopomer in self.isotopomers[f"{substrate}"]:
            if isotopomer.labelling == labelling and isotopomer.name == substrate:
                self.isotopomers[f"{substrate}"].remove(isotopomer)

    def generate_combinations(self):
        """
        Generate all possible combinations of labelled substrates 
        using the LabelInput class.
        """
        # Check if "self.isotopomers" is not empty
        if not self.isotopomers:
            raise ValueError("No isotopomers have been added. Please add at least one isotopomer.")
        # Check if there is at least one isotopomer for each substrate
        for substrates, isotopomers in self.isotopomers.items():
            if not isotopomers:
                raise ValueError(f"No isotopomers for {substrates}. Please add at least one isotopomer.")
        
        # Initialize the LabelInput object with the isotopomers dictionary
        self.label_input = LabelInput(self.isotopomers)
        logger.info(f"Label Input - {self.label_input}")

        self.label_input.generate_labelling_combinations()
        
        logger.debug(
            f"Isotopomers combinations:"
            f"{self.label_input.isotopomer_combinations}\n"
        )
       
    def create_tmp_folder(self):
        """ 
        Create a temp folder to store all the files generated by IsoDesign.

        """

        self.tmp_folder_path = Path(f"{self.output_folder_path}/{self.model_name}_tmp")
        self.tmp_folder_path.mkdir(parents=True, exist_ok=True)

        # logger.info(f"Results folder path '{tmp_folder_path}'.\n")

    def clear_tmp_folder(self):
        """
        Clear the temp folder containing all the files generated by IsoDesign.
        """
       
        shutil.rmtree(Path(f"{self.output_folder_path}/{self.model_name}_tmp"))
        
    def clear_previous_results(self):
        """
        Clear the "_res" folder containing the results of the previous run.
        """
        for folder in self.tmp_folder_path.iterdir():
            if folder.is_dir() and folder.name.endswith("_res"):
                shutil.rmtree(folder)
    
    def clear_previous_linp(self):
        """
        Clear the ".linp" files generated by the previous run.    
        """
        for linp_files in self.tmp_folder_path.iterdir():
            if linp_files.is_file() and linp_files.name.endswith(".linp"):
                os.remove(linp_files)
                
    def get_isotopomer_price(self, isotopomer_labelling, isotopomer_name):
        """ 
        Get the price of an isotopomer based on its labelling and name 
        from the isotopomers dictionary.
        
        :param isotopomer_labelling: isotopomer labelling
        :param isotopomer_name: isotopomer name
        """

        for isotopomers_list in self.isotopomers.values():
            for isotopomer in isotopomers_list:
                if isotopomer.labelling == isotopomer_labelling and isotopomer.name == isotopomer_name:
                    return isotopomer.price
    
    def configure_linp_files(self):
        """
        This method configures the structure and content of future 
        linp files, a TSV format used for labelled simulations, 
        grouping label input forms and their fractions. 
        Dataframes are generated in linp format, each containing
        a specific combination of labelled substrates. These dataframes 
        are then stored and used to create the final linp files.

        """
        self.linp_dataframes = {}

        # Calculate the number of digits based on the total number of files
        total_files = len(self.label_input.isotopomer_combinations["All_combinations"])
        num_digits = len(str(total_files))
        
        # Generate a dataframe for each pair of isotopomers 
        for index, pair in enumerate(self.label_input.isotopomer_combinations["All_combinations"], start=1):
            df = pd.DataFrame({'Id': None,
                                'Comment': None,
                                'Specie': self.label_input.names,
                                'Isotopomer': self.label_input.labelling_patterns,
                                'Value': pair.astype(float)})

            # remove rows with value = 0
            # df = df.loc[df["Value"] != 0]
            
            # add a column "Price" containing the price of each isotopomer multiplied by its fraction
            # applies the 'get_isotopomer_price' method to each row (axis=1) in the dataframe 'df'
            df["Price"] = df.apply(lambda x: self.get_isotopomer_price(x["Isotopomer"], x["Specie"]), axis=1) * df["Value"]
            # logger.debug(f"Folder path : {self.tmp_folder_path}\n Dataframe {index:02d}:\n {df}")
            
            # Store the dataframe in the linp_dataframes dictionary
            # key : ID_{index}, value : dataframe in dictionary format
            self.linp_dataframes.update({f"ID_{index:0{num_digits}d}": df.to_dict(orient="list")})            
            

    def remove_linp_configuration(self, index_to_remove:list):
        """
        Removes linp DataFrames from the linp_dataframes 
        list according to the indices specified in the 
        list provided. 

        :param index_to_remove: list of indices to remove 
                                from linp_dataframes
        """   
        to_remove = []
        
        for index, (key, value) in enumerate(self.linp_dataframes.items()):
            if index in index_to_remove:
                # Store the linp DataFrames to remove in the linp_to_remove dictionary
                # key : index, value : dictionary containing the id of the linp DataFrame as key and its content as value
                # example : {0: {'ID_01': {'Id': None, 'Comment': None, 'Specie': ['Gluc', 'Gluc'], 'Isotopomer': ['111111', '100000'], 'Value': [0.5, 0.5], 'Price': [10.0, 10.0]}}
                self.linp_to_remove[index] = {key: value}
                to_remove.append(key)
            
        for id in to_remove:
            logger.info(f"Combination(s) removed : {id}\n") 
            logger.debug(f"{id} : {self.linp_dataframes[id]}\n")
            del self.linp_dataframes[id]  
             
    
    def reintegrate_linp_configuration(self, index_to_reintegrate:list):
        """
        Reintegrates linp DataFrames into the linp_dataframes 
        list according to the indices specified in the 
        list provided. 

        :param index_to_reintegrate: list of indices to reintegrate 
                                    into linp_dataframes.
        """
        # Get the keys of the linp DataFrames to reintegrate
        for index in index_to_reintegrate:
            for key, value in self.linp_to_remove[index].items():
                items = list(self.linp_dataframes.items())
                items.insert(index, (key, value))
                self.linp_dataframes = dict(sorted(items))
                logger.info(f"Combination reintegrated : {key}\n")
                logger.debug(f"{key} : {value}\n")
            del self.linp_to_remove[index]

    def generate_linp_files(self):
        """
        Generates linp files (TSV format) in the temp folder (self.tmp_folder_path). 
        Each file contains a combination of labelled substrates. 
        
        A file is generated containing a mapping that associates each file 
        number with its respective combinations. 

        """

        # create mapping to associate file number with its respective combinations
        with open(os.path.join(str(self.output_folder_path), f'{self.model_name}_IDs_combinations.tsv'), 'w', encoding="utf-8") as f:
            # Write file column names
            f.write("ID\t" + 
                    "\t".join([f"{specie}_{isotopomer}" for specie, isotopomer in zip(self.label_input.names, self.label_input.labelling_patterns)]) + 
                    "\tPrice\n")
            for index, dataframes in self.linp_dataframes.items():
                # Dataframes are stored in dictionary format
                df = pd.DataFrame.from_dict(dataframes)
                # Write the content of the linp files in the tsv file
                f.write(f"{index}\t" + 
                        "\t".join(map(str, df["Value"])) +
                        f"\t{df['Price'].sum()}\n")
                # Remove rows with value = 0 (no values equal to 0 in ".linp" files)
                df = df.loc[df["Value"] != 0]
                df.to_csv(os.path.join(str(self.tmp_folder_path), f"{index}.linp"), sep="\t", index=False)
                # Store the number of labeled inputs and the total price of each linp file in the linp_infos dictionary
                # It will be used in the rating criteria.
                # key : index, value : namedtuple containing the number of labeled inputs and the total price
                self.linp_infos[f"{index}"] = linp_info(len([isotopomer for isotopomer in df["Isotopomer"] if "1" in isotopomer]), 
                                                                df["Price"].sum())
             
        self.vmtf_element_dict["linp"] = [f"{index}" for index in self.linp_dataframes.keys()]

        logger.info(f"{len(self.vmtf_element_dict['linp'])} linp files have been generated in {self.tmp_folder_path}.")
        

    def generate_vmtf_file(self):
        """
        Generate a vmtf file (TSV format) that permit to combine variable
        and constant parts of a network model.
        This file contained columns using imported files extensions. Each row contains
        imported files names that will be used to produce a ftbl file used in the calculation. 
        Each row have ftbl column with unique and non empty name. 
        """

        # Value convert into Series 
        # Permit to create a dataframe from a dictionary where keys have different length of value   
        df = pd.DataFrame.from_dict(self.vmtf_element_dict)
        # Add a new column "ftbl" containing the values that are in the key "linp" of vmtf_element_dict
        # These values will be the names of the export folders of the results  
        df["ftbl"] = self.vmtf_element_dict["linp"]
        logger.debug(f"Creation of the vmtf file containing these files :\n {df}")
        df.to_csv(f"{self.tmp_folder_path}/{self.model_name}.vmtf", sep="\t", index=False)

        # logger.info(f"Vmtf file has been generated in '{self.tmp_folder_path}.'\n")


    def influx_simulation(self, param_list):
        """
        Run the simulation using the specified influx_si mode (stationary or instationary).

        :param param_list: List of command-line arguments to pass to influx_si.
        :return: A Popen object representing the running subprocess.
        """
        # Change directory to the folder containing all the file to use for influx_si
        os.chdir(self.tmp_folder_path)
        
        self.command_list = param_list
        logger.info(f"Command to run: {self.command_list}")

        result = subprocess.Popen(self.command_list, 
                                    stderr=subprocess.PIPE,
                                    text=True)
        return result
    

    def check_err_files(self):
        """ 
        Check if, at the end of calculations with influx_si, ".err" files are 
        empty. If they are not, return the file names and contents.
        """
        err_file_list = []
        for root, dirs, files in os.walk(self.tmp_folder_path):
            for file in files:
                if file.endswith(".err"):
                    # Check if the file is not empty
                    if os.stat(os.path.join(root, file)).st_size > 0:
                        err_file_list.append(file)

            if err_file_list:
                for err_files in err_file_list:
                    with open(os.path.join(root, err_files), 'r') as f:
                        err_file_content = f.read()
                        logger.error(f"Error file {err_files} : {err_file_content}")
                        raise Exception(f"Error file {err_files} : {err_file_content}")
          
    def generate_summary(self):
        """
        Read the tvar.sim files and generate a summary dataframe containing :
            - flux names, flux types,
            - the flux values in the tvar.sim files, 
            - the difference between the flux values in the input tvar file and the tvar.sim files,
            - flux SDs in each tvar.sim file
        
        The summary dataframe is generated in an excel file in the results folder.
        During the simulation, fluxes are added. These are highlighted in the summary dataframe.
        
        """

        # use os.walk to generate the names of folders, subfolders and files in a given directory
        self.tvar_sim_paths = [Path(f"{root}/{files}") 
                              for (root, _ , files_names) in os.walk(self.tmp_folder_path) 
                                for files in files_names if files.endswith('.tvar.sim')]
        
        logger.debug(f"List of tvar.sim file paths : \n {self.tvar_sim_paths}")
        # get the number of structurally identified fluxes from each "tvar.sim" files 
        # Fluxes are considered structurally identified if they have an SD <= 10000 
        self.structures_identified={
            file_path.name.split(".")[0]: sum([len(struct) for struct in pd.read_csv(file_path, sep="\t", usecols=["Struct_identif"]).values if struct == 'yes']) 
            for file_path in self.tvar_sim_paths}

        # dictionary containing columns "Name", "Kind" and "SD" from each tvar.sim file
        tvar_sim_dataframes = {
            f'{file_path.name.split(".")[0]}': pd.read_csv(
                file_path, sep="\t", usecols=["Name", "Kind", "SD"], index_col=["Name", "Kind"]).rename(
                    columns={"SD": f"{file_path.name.split('.')[0]}"})
            for file_path in self.tvar_sim_paths
        }
        tvar_sim_dataframes = pd.concat(tvar_sim_dataframes.values(), axis=1, join="inner")
        logger.debug(f"tvar_sim_dataframes: {tvar_sim_dataframes}")

        # take the flux values from the first tvar.sim file 
        # flux values are the same in all tvar.sim files
        tvar_sim_values = pd.read_csv(self.tvar_sim_paths[0], sep="\t", usecols=["Value", "Name", "Kind"]) 

        # take the "Name", "Kind" and "Value" columns from the input tvar file
        input_tvar_file = self.mtf_files['tvar'].data[["Name", "Kind", "Value"]]
        # merge data from the input tvar file with data from tvar.sim files based on flux names and kind
        merged_tvar = pd.merge(input_tvar_file, tvar_sim_values, on=["Name", "Kind"], how="outer", suffixes=('_init', None))
        merged_tvar["Value difference"] = merged_tvar["Value_init"] - merged_tvar["Value"]
        logger.debug(f"Merged_tvar_values : {merged_tvar}")

        # merge the "merged_tvar" dataframe with concatenated dataframes from the "tvar_sim_dataframes" dataframes
        # delete the "Value_tvar_init" column, which is not required 
        self.summary_dataframe = pd.merge(merged_tvar, tvar_sim_dataframes, on=["Name","Kind"]).rename(columns={"Value_init": " Intial flux value"})
        logger.debug(f"Summary dataframe present in '{self.output_folder_path}' : {self.summary_dataframe}\n")

        # Creating a Styler object for the summary_dataframe DataFrame
        summary_dataframe_styler=self.summary_dataframe.style.apply(
            # If at least one value is missing in the row, set the style with a pale yellow background color
            # Repeating the style for each cell in the row
            lambda row: ['background-color: #fffbcc' if row.isnull().any() else '' for _ in row],
            # Applying the lambda function along the rows of the DataFrame
            axis=1)
        summary_dataframe_styler.to_excel(f"{self.output_folder_path}/{self.model_name}_summary.xlsx", index=False)


    def data_filter(self, fluxes_names:list=None, kind:list=None, pathways:list=None):
        """
        Filters summary dataframe by fluxes names, kind and/or metabolic pathway 

        :param fluxes_names: list of fluxes names to be displayed 
        :param kind: "NET", "XCH", "METAB"
        :param pathway: name of metabolic pathways to be displayed  

        :return: filtered dataframe
        """
        
        self.filtered_dataframe = self.summary_dataframe.copy()

        if fluxes_names:
            # keep only rows with fluxes names in the list
            self.filtered_dataframe = self.filtered_dataframe.loc[self.filtered_dataframe["Name"].isin(fluxes_names)]
        if kind:
            # keep only rows with kind in the list
            self.filtered_dataframe = self.filtered_dataframe.loc[self.filtered_dataframe["Kind"].isin(kind)]
        if pathways:
            # list storing all fluxes concerned by the selected metabolic pathway(s) 
            all_fluxes = []
            for pathway_name in pathways:
                # key "pathway" in netan dictionary contains another dictionary as value
                # In netan dictionary, key : pathway names, value : list with fluxes names involved in this pathway as value
                all_fluxes.extend(self.netan["pathway"][pathway_name])
        
            self.filtered_dataframe = self.filtered_dataframe.loc[self.filtered_dataframe["Name"].isin(all_fluxes)]
        # Store the filters used in the analysis
        self.filters = {
            "fluxes_names": fluxes_names,
            "kind": kind,
            "pathways": pathways}
        # logger.info(f"Filtered dataframe :\n{self.filtered_dataframe}")

        # return self.filtered_dataframe

    def generate_score(self, method :list, operation = None, **kwargs):
        """
        Generate a score for each labelled substrate combination according 
        on the criteria method(s) applied.

        :param method: list of criteria methods to apply
        :param operation: operation to apply to the scores (Addition, Multiply, Divide)
        :param kwargs: additional arguments for the rating methods
        """
        score_object = ScoreHandler(self.filtered_dataframe.iloc[:, 5:] if self.filtered_dataframe is not None else self.summary_dataframe.iloc[:, 5:])
        
        score_object.apply_criteria(method, **kwargs)
        if operation:
            score_object.apply_operations(operation)

        # Store the scores in a dataframe    
        self.scores = pd.DataFrame.from_dict(score_object.columns_scores, 
                                   orient='index')
        # Store the criteria, parameters and operations used to generate the scores
        self.selected_criteria = method
        self.criteria_parameters = kwargs
        self.applied_operations = operation

        # logger.debug(f"Scores applied to the dataframe :\n{score_object.columns_scores}")
    
    def apply_log(self):
        """
        Apply a log of 10 to the values contained in 
        the score table (stored in the self.scores attribute).
        """
        self.scores=np.log10(self.scores)

    def register_scores(self, number, block_name):
        """
        Stores the analysis data (dataframe, criteria, criteria parameters,
        score table, filters and operations) in a dictionary.
        This method updates the all_scores dictionary with a new key
        identified by the specified "number".

        :param number: number corresponding to the analysis 
        """
        self.all_scores.update(
            {number: {
                "name" : block_name,
                "dataframe": self.filtered_dataframe,
                "filters": self.filters,
                "criteria": self.selected_criteria,
                "criteria_parameters": self.criteria_parameters,
                "columns_scores": self.scores, 
                "applied_operations": self.applied_operations,
            }}
        )  

        logger.debug(self.all_scores)             

    def export_data(self, number, figure):
        """
        Export the results of the analysis to tsv files and an image file (html format).

        :param number: number corresponding to the analysis
        :param figure: plotly figure object
        """
        res_folder_path = Path(f"{self.output_folder_path}/{self.all_scores[number]["name"]}_res")
        res_folder_path.mkdir(parents=True, exist_ok=True)

        # Export the dataframe and the scores table to tsv files
        self.all_scores[number]["dataframe"].to_csv(f"{res_folder_path}/{self.all_scores[number]["name"]}_dataframe.tsv", index=False, sep="\t")
        self.all_scores[number]["columns_scores"].to_csv(f"{res_folder_path}/{self.all_scores[number]["name"]}_scores.tsv", index=True, sep="\t")

        figure.write_html(f"{res_folder_path}/{self.all_scores[number]["name"]}_barplot.html")


# if __name__ == "__main__":

#     test = Process()
#     test.get_path_input_netw(r"c:\Users\kouakou\Documents\test_data\design_test_1.netw")
#     test.output_folder_path = r"c:\Users\kouakou\Documents\test_data"
#     test.load_model()
#     test.analyse_model()
#     test.create_tmp_folder()

#     test.configure_unlabelled_form()
#     test.add_isotopomer("Gluc", "100000", 10, 0, 100)
#     test.add_isotopomer("Gluc", "111111", 10, 0, 100)
#     test.generate_combinations()

#     test.configure_linp_files()
#     test.generate_linp_files()
#     test.generate_vmtf_file()
#     test.copy_files()
    
#     # test.clear_previous_run()
#     command_list = ["--prefix", test.model_name, "--emu", "--ln", "--noopt"]
#     # test.influx_simulation(command_list, influx_mode="influx_s")
    
#     test.generate_summary()
#     # test.save_process_to_file()
#     test.data_filter(pathways=["GLYCOLYSIS"], kind=["NET"])
#     test.generate_score(["sum of SDs", "number of fluxes with SDs < threshold"], threshold=1, operation="Multiply")
# #     # test.draw_barplot(test.scores)
# #     # test.register_scores(1)

#     # test.export_data("score_1")

    

